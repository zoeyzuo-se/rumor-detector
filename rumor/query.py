# scripts/query.py
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from config import *

def main():
    # ç”¨æˆ·è¾“å…¥å†…å®¹
    query = input("è¯·è¾“å…¥ä½ æƒ³é‰´åˆ«çš„è¯ï¼ˆå¦‚ï¼šè½¬å‘è¿™æ¡è§†é¢‘èƒ½è¾Ÿé‚ªï¼‰ï¼š\n> ")

    # åµŒå…¥æ¨¡å‹
    embeddings = AzureOpenAIEmbeddings(
        model=EMBEDDING_DEPLOYMENT_NAME,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        api_key=AZURE_OPENAI_API_KEY,
        openai_api_version=EMBEDDING_API_VERSION,
    )

    # åŠ è½½ FAISS å‘é‡åº“
    vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

    # åˆå§‹åŒ– GPT æ¨¡å‹
    llm = AzureChatOpenAI(
        azure_deployment="gpt-4",  # éƒ¨ç½²çš„ GPT æ¨¡å‹åå­—
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        api_key=AZURE_OPENAI_API_KEY,
        api_version="2024-12-01-preview"
    )

    # æ„å»ºé—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        chain_type="stuff",
        return_source_documents=True
    )

    # æ‰§è¡ŒæŸ¥è¯¢
    result = qa_chain.invoke({"query": query})
    print("\nğŸ“¢ å›ç­”ï¼š")
    print(result["result"])

    # print("\nğŸ“š å‚è€ƒå†…å®¹ç‰‡æ®µï¼š")
    # for doc in result["source_documents"]:
    #     print(f"---\n{doc.page_content.strip()}")

if __name__ == "__main__":
    main()
